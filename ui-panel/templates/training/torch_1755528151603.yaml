apiVersion: sagemaker.amazonaws.com/v1
kind: HyperPodPyTorchJob
metadata:
  labels:
    app.kubernetes.io/name: HyperPod
    app.kubernetes.io/managed-by: kustomize
    app: torchrecipe-1
  name: torchrecipe-1
spec:
  nprocPerNode: "2"
  replicaSpecs:
    - name: pods
      replicas: 2
      template:
        spec:
          serviceAccountName: mlflow-service-account
          # serviceAccountName: hp-training-operator-pod-manager
          nodeSelector:
            beta.kubernetes.io/instance-type: ml.g5.12xlarge
          containers:
            - name: pytorch
              image: 633205212955.dkr.ecr.us-west-2.amazonaws.com/sm-training-op-torch26-smhp-op-v2:latest
              imagePullPolicy: Always
              ports:
                - containerPort: 8080 # HyperPodElasticAgent port
              resources:
                requests:
                  nvidia.com/gpu: 2
                  vpc.amazonaws.com/efa: 1
                limits:
                  nvidia.com/gpu: 2
                  vpc.amazonaws.com/efa: 1
              env:
              - name: LOGLEVEL
                value: "INFO"
              - name: FI_PROVIDER
                value: "efa"
              # - name: TORCH_DISTRIBUTED_DEBUG
              #   value: "DETAIL"
              # - name: TORCH_NCCL_ENABLE_MONITORING
              #   value: "1"
              # - name: TORCH_NCCL_TRACE_BUFFER_SIZE
              #   value: "20000"
              - name: NCCL_DEBUG
                value: "DEBUG"
              # - name: NCCL_SOCKET_IFNAME
              #   value: "^lo"
              - name: NCCL_SOCKET_IFNAME
                value: "eth0"
              # - name: TORCH_NCCL_ASYNC_ERROR_HANDLING
              #   value: "1"
              # - name: HF_TOKEN
              #   value: "12345"
              - name: MLFLOW_TRACKING_URI
                value: arn:aws:sagemaker:us-west-2:633205212955:mlflow-tracking-server/pdx-mlflow
              - name: MLFLOW_EXPERIMENT_NAME
                value: torchrecipe-1
              - name: MLFLOW_TAG_NPROCPERNODE
                value: "2"
              - name: MLFLOW_TAG_REPLICAS
                value: "2"
              - name: MLFLOW_TAG_EFAPERNODE
                value: "1"
              - name: MLFLOW_TAG_INSTANCETYPE
                value: ml.g5.12xlarge
              - name: TORCH_RECIPE_PY_PATH
                value: /s3/training_code/model-training-with-hyperpod-training-operator/gptnx-fsdp/src/train.py
              - name: TORCH_RECIPE_PY_PARAMS
                value: --max_context_width=2048 --num_key_value_heads=8 --intermediate_size=1024 --hidden_width=512 --num_layers=32 --num_heads=16 --model_type=gpt_neox --tokenizer=EleutherAI/gpt-neo-1.3B --checkpoint_freq=50 --validation_freq=100 --max_steps=50000 --checkpoint_dir=/dfsx/gptnx/checkpoints --dataset=allenai/c4 --dataset_config_name=en --resume_from_checkpoint=/dfsx/gptnx/checkpoints --train_batch_size=1 --val_batch_size=1 --sharding_strategy=full --offload_activations=1
              command: ["bash", "/docker_workspace/torch_recipe_dist_run.sh"]
              volumeMounts:
                - name: persistent-storage-s3
                  mountPath: /s3
                  readOnly: false
                - name: shmem
                  mountPath: /dev/shm
                - name: local
                  mountPath: /local
                - name: inst-nvme
                  mountPath: /ckpt-path
                - name: local-cache
                  mountPath: /root/.cache
                # - name: persistent-storage
                #   mountPath: /dfsx
          volumes:
            - name: shmem
              hostPath: 
                path: /dev/shm
            - name: local
              hostPath:
                path: /mnt/k8s-disks/0
            - name: local-cache
              hostPath:
                path: /opt/dlami/nvme/.cache
            - name: inst-nvme
              hostPath:
                path: /opt/dlami/nvme/checkpoints/
            # - name: persistent-storage
            #   persistentVolumeClaim:
            #     claimName: fsx-claim
            - name: persistent-storage-s3
              persistentVolumeClaim:
                claimName: s3-claim
  runPolicy:
    jobMaxRetryCount: 5
    restartPolicy:
      numRestartBeforeFullJobRestart: 3
      evalPeriodSeconds: 21600
      maxFullJobRestarts: 1
    cleanPodPolicy: "All"
